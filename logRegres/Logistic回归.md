###Logistic回归的一般过程

 1. 收集数据：采用任意方法收集数据
 2. 准备数据：需要计算距离，要求数据为数值型，结构化数据最佳
 3. 分析数据：采用任意方式对数据进行分析
 4. 训练算法：大部分时间用于训练，训练的目的是为了找到最佳的分类回归系数
 5. 测试算法：一旦训练步骤完成，分类将会很快
 6. 使用算法：输入一些数据，将其转化成对应的结构化数据，然后基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定类别。

###Sigmoid函数
Sigmoid函数的具体计算公式：
$$
\sigma(z)=\frac {1} {1+e^{-z}}
$$其中 $z=w_{0}x_{0}+w_{1}x_{1}+...+w_{n}x_{n}$ ，$w$为系数，$x$ 为特征。为了实现logistic回归分类器，我们可以在每个特征上乘以一个回归系数，然后把所有的结果值相加，将这个总和结果代入sigmoid函数中，进而得到一个范围在0~1之间的数值。任何大于0.5的数据被分入1类，小于0.5的数据被归入0类。所以，logistic回归也可以被看成是一种概率估计。

如果采用向量的写法，上述公式写成：$z=W^{T}X$，表示将这两个数值向量对应元素相乘然后全部加起来得到$z$值。其中向量 $x$ 是分类器的输入数据， 向量 $w$ 就是我们要找的最佳参数。

###梯度上升法
基本思想：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探索。如果梯度记为 $\bigtriangledown$ ，则函数 $f(x,y)$ 的梯度由下式表示：
$$
\bigtriangledown f(x,y)= \begin{pmatrix} 
\frac{\partial f(x,y)}{\partial x}  \\
\frac{\partial f(x,y)}{\partial y} 
\end{pmatrix}$$
这个梯度意味着要沿$x$ 方向移动 $\frac{\partial f(x,y)}{\partial x}$ ，沿 $y$ 方向移动 $\frac{\partial f(x,y)}{\partial y}$ ，其中 $f(x,y)$ 必须要在待计算的点上有定义并且可微。
梯度算子总是指向函数值增长最快的方向，这里是移动方向，而移动量的大小称为步长，记作 $\alpha$ ，用向量来表示的话，梯度上升算法的迭代公式为：
$$
w:=w+\alpha \bigtriangledown _{w}f(w)
$$该公式将一直被迭代下去，直到某个条件停止为止，比如迭代次数达到某个值或算法达到某个可以允许的误差范围。梯度下降只是将上述中的加号改为减号。梯度上升算法用来求函数的最大值，而梯度下降算法用来求函数的最小值。

###训练算法：随机梯度上升
梯度上升算法在每次更新回归系数时都需要遍历整个数据集，该方法处理100个左右的数据集尚可，但处理数十亿样本和成千上万的特征，此方法的计算复杂度太高。一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为随机梯度上升算法。由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习算法。与"在线学习"相对应，一次处理所有数据被称作是"批处理"。

###小结
Logistic回归的目的是寻找一个非线性函数sigmoid的最佳拟合参数，求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升算法。
随机梯度上升算法和梯度上升算法的效果相当，但占用更少的计算资源。此外，随机梯度是一种在线算法，可以在数据到来时就完成参数的更新，而不需要重新读取整个数据集来进行批处理运算。


