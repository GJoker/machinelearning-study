###决策树的原理
决策树是用样本的属性作为结点，用属性的取值作为分支的树结构。 

决策树的根结点是所有样本中信息量最大的属性。树的中间结点是该结点为根的子树所包含的样本子集中信息量最大的属性。决策树的叶结点是样本的类别值。决策树是一种知识表示形式，它是对所有样本数据的高度概括决策树能准确地识别所有样本的类别，也能有效地识别新样本的类别。

###ID3 
决策树算法ID3的基本思想：
首先找出最有判别力的属性，把样例分成多个子集，每个子集又选择最有判别力的属性进行划分，一直进行到所有子集仅包含同一类型的数据为止。最后得到一棵决策树。

ID3算法流程：

 1. 对当前例子集合，计算各属性的信息增益；
 2. 选择信息增益最大的属性Ak；
 3.  把在Ak处取值相同的例子归于同一子集，Ak取几个值就得几个子集；
 4. 对既含正例又含反例的子集，递归调用建树算法；
 5.  若子集仅含正例或反例，对应分枝标上P或N，返回调用处。

一般只要涉及到树的情况，经常会要用到递归。

信息熵的计算：
$$
H(S)=-\sum_{i=1}^{n}p(x_{i})log_{2}p(x_{i})
$$$n$ 是分类数目，$p(x_{i})$ 是选择 $i$ 分类的概率。

信息增益计算公式：
$$
Gain(S,A)=Entropy(S)-\sum_{v\in Value(A)}^{}\frac{\left | S_{v} \right |}{\left |S\right |}Entropy(S_{v})
$$其中 $A$ 是属性，$Value(A)$ 是属性 $A$ 的取值集合， $v$ 是 $A$ 的某一属性值， $S_{v}$ 是 $S$ 中 $A$ 的值为 $v$ 的样例集合，$\left | S_{v} \right |$  为 $S_{v}$ 中所含样例数。 

