kNN和决策树都给出了“该数据实例属于哪一类”这类问题的明确答案，而有时候的分类并不能给出明确的答案，使用概率论进行分类的方法。

###概述
**朴素贝叶斯模型(NaiveBayesian classification))：**

---
$V_{map}=arg$  $max P( V_{j} | a_{1},a_{2}...a_{n})$

$V_{j}$属于$V$集合

其中$V_{map}$是给定一个 example,得到的最可能的目标值.

其中$a_{1},a_{2}...a_{n}$是这个example里面的属性.

这里面,$V_{map}$目标值,就是后面计算得出的概率最大的一个，所以用$max$来表示。

---
贝叶斯公式应用到 $P( V_{j} | a_{1},a_{2}...a_{n})$中.

可得到$V_{map}=arg$  $max P(  a_{1},a_{2}...a_{n}|V_{j} )$ $P(V_{j}) / P(  a_{1},a_{2}...a_{n})$

又因为朴素贝叶斯分类器默认$a_{1},a_{2}...a_{n}$他们互相独立的.

所以 $P(  a_{1},a_{2}...a_{n})$对于结果没有用处. [*因为所有的概率都要除同一个东西之后再比较大小,最后结果也似乎影响不大*]

可得到$V_{map}=arg$  $max P(  a_{1},a_{2}...a_{n}|V_{j} )$ $P(V_{j}) $

然后:

"朴素贝叶斯分类器基于一个简单的假定：给定目标值时属性之间相互条件独立。换言之。该假定说明给定实例的目标值情况下。观察到联合的$a_{1},a_{2}...a_{n}$的概率正好是对每个单独属性的概率乘积：

$$ P(a_{1},a_{2}...a_{n}| V_{j} ) =\prod_{i}^{n} P( a_{i}| V_{j} )$$
....
朴素贝叶斯分类器：$V_{nb}=arg$ $max P( V_{j} ) \prod_{i}^{n} P( a_{i}| V_{j} )$

其中$a_{1},a_{2}...a_{n}$为特征值，$V_{j}$为分类的结果。这也体现了贝叶斯决策理论的核心思想，即选择具有最高概率的决策。它是**文档**分类的常用算法。

###朴素贝叶斯的一般过程

 1. 收集数据：可以使用任何方法。如RSS源
 2. 准备数据：需要数值型或者布尔型数据
 3. 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好
 4. 训练算法：计算不同的独立特征条件概率
 5. 测试算法：计算错误率
 6. 使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意场景中使用朴素贝叶斯分类器，不一定是文本

###伪代码
**计算每个类别中的文档数目**
**对每篇训练文档：** 

- **对每个类别：**
  - **如果词条出现在文档中-->增加该词条的计数值**
  - **增加所有词条的计数值**
 
- **对每个类别：**
  - **对每个词条：**
     - **将该词条的数目除以总词条数目得到条件概率**
- **返回每个类别的条件概率**

  

  
 